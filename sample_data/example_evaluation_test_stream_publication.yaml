shortname: genLLMchatArena     #Author should put effort into ensureing this shortname is unique. It should be short as it will be typed and filtered on 
title: LLM question answer with blind A vs B comparison to get ELO score
specification_mutable_url :  https://github.com/lm-sys/FastChat/blob/main/docs/arena.md  # contents behind this url can evolve over time
specification_current_cid: 0x6ACB287c943C229c550F350B07e7B4c9f0212333    
specification_all_cid_versions: 
  - 0x6FCB287c943C1119c550F350B07e7B4c9f026001   
  - 0x6FCB287c943C1119c550F350B07e7B4c9f026000
  - 0x6ACB287c943C229c550F350B07e7B4c9f0212333
paper_pdf: https://arxiv.org/pdf/2403.04132.pdf   #optional 
paper_doi: 10.48550/arXiv.2403.04132  #Optionally they could write the full url   https://doi.org/10.48550/arXiv.2403.04132 . DOI's are super good b/c  they are permanent and guranteed to be unique
paper_BibTex: |                                   #optional       
  @misc{chiang2024chatbotarenaopenplatform,
  title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference}, 
  author={Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},
  year={2024},
  eprint={2403.04132},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2403.04132}, 
  }
input_dimensions:
  - input_name : prompt_text
    input_type : string
evaluation_dimensions: 
  - eval_name: BLUE 
    eval_type: float
  - eval_name: ResponseTime
    eval_type: int
aggregate_metrics:
  - metric_name: average_BLUE
    metric_type: float
    metric_higher_is_better: true
  - metric_name: averageRoundReponseTIme
    metric_type: int_ms
    metric_higher_is_better: false
max_response_time_ms: 10000  #optional 
minimum_sample_size: 100  #optional
test_data_publicaion_required_within: 6months #optional 
